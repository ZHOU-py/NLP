{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cornell_Movies-Dialogs_Corpus S2S 聊天机器人.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1b3wYiXMf8Qc7aDgYdELQowr2lv_72XYd",
      "authorship_tag": "ABX9TyO0a7Cb2oJOZ/221iGYYAuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZHOU-py/NLP/blob/master/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/Cornell_Movies_Dialogs_Corpus_S2S_%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhgY1QnSUEjB"
      },
      "source": [
        "#### 下载数据文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DynCqeQOUDvm"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32CYrlBG9Mxz"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K74sOYNdbj_"
      },
      "source": [
        "#### 加载和预处理数据\n",
        "- 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
        "- involves 9,035 characters from 617 movies\n",
        "- in total 304,713 utterances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKxDM889VRNa",
        "outputId": "a8e67309-588d-41f4-ac18-816e13603479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corpus_name = \"cornell movie-dialogs corpus\" \n",
        "corpus = os.path.join(\"data\", corpus_name)\n",
        "def printLines(file, n=10):\n",
        "  with open(file, 'rb') as datafile:\n",
        "    lines = datafile.readlines() \n",
        "  for line in lines[:n]:\n",
        "    print(line)\n",
        "printLines(os.path.join(corpus, \"movie_lines.txt\"))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
            "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
            "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
            "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
            "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
            "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
            "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
            "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
            "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
            "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMBdp3k02gcF"
      },
      "source": [
        "##### 创建格式化数据\n",
        "解析原始数据文件 movie_lines.txt\n",
        "* `loadLines`：将文件的每一行拆分为字段（lineID, characterID, movieID, character, text)组合的字典\n",
        "* `loadConversations`: 根据`movie_conversations.txt`将`loadLines`中的每一行数据进行归类\n",
        "* `extractSentencePairs`: 从对话中提取句子对\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-M2IVvWWIwO"
      },
      "source": [
        "# 将文件的每一行拆分为字段字典\n",
        "def loadLines(fileName, fields):\n",
        "  lines = {}\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\" +++$+++ \")\n",
        "      lineObj = {}\n",
        "      for i, field in enumerate(fields):\n",
        "        lineObj[field] = values[i]\n",
        "      lines[lineObj['lineID']] = lineObj\n",
        "  return lines"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfF5h6HPdA5w"
      },
      "source": [
        "#  将 'loadLines'中的行字段分组为基于 *movie_conversations.txt* 的对话\n",
        "def loadConversations(fileName, lines, fields):\n",
        "  conversations = []\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\" +++$+++ \")\n",
        "      # Extract fields\n",
        "      convObj = {}\n",
        "      for i, field in enumerate(fields):\n",
        "        convObj[field] = values[i]\n",
        "      # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485','L598486', ...]\")\n",
        "      lineIds = eval(convObj[\"utteranceIDs\"])\n",
        "      # Reassemble lines\n",
        "      convObj[\"lines\"] = []\n",
        "#      print(lineIds)\n",
        "      for lineId in lineIds:\n",
        "        convObj[\"lines\"].append(lines[lineId])\n",
        "      conversations.append(convObj)\n",
        "\n",
        "  return conversations"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCQmH3y5i-V"
      },
      "source": [
        "# 从对话中提取一对句子\n",
        "def extractSentencePairs(conversations):\n",
        "  qa_pairs = []\n",
        "  for conversation in conversations:\n",
        "    for i in range(len(conversation[\"lines\"])-1):\n",
        "      inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "      targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "      if inputLine and targetLine:\n",
        "        qa_pairs.append([inputLine,targetLine])\n",
        "  return qa_pairs"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evYeZatM8CRz",
        "outputId": "7fa8522a-8336-4d56-b8b0-d46bca6279ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# 定义新文件的路径\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\") \n",
        "delimiter = '\\t'\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "# 初始化行dict，对话列表和字段ID\n",
        "lines = {}\n",
        "conversations = []\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"] \n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "# 加载行和进程对话\n",
        "print(\"\\n Processing corpus...\")\n",
        "lines = loadLines(os.path.join(corpus,\"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "print(\"\\n Loading conversations...\")\n",
        "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\\\n",
        "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "# 写入新的csv文件\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "  writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n') \n",
        "  for pair in extractSentencePairs(conversations):\n",
        "    writer.writerow(pair)\n",
        "\n",
        "# 打印一个样本的行\n",
        "print(\"\\nSample lines from file:\") \n",
        "printLines(datafile)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Processing corpus...\n",
            "\n",
            " Loading conversations...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-35ff52bb6c22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadLines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"movie_lines.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVIE_LINES_FIELDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Loading conversations...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadConversations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"movie_conversations.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVIE_CONVERSATIONS_FIELDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 写入新的csv文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-e460e619ed70>\u001b[0m in \u001b[0;36mloadConversations\u001b[0;34m(fileName, lines, fields)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#      print(lineIds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlineId\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlineIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mconvObj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lines\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mconversations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'L36785'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xes_FfCCUKU2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb936tzk98JP"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}