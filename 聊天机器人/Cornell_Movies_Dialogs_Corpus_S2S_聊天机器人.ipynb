{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cornell_Movies-Dialogs_Corpus S2S 聊天机器人.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JhgY1QnSUEjB",
        "2K74sOYNdbj_",
        "bny_XSEdEmWw"
      ],
      "mount_file_id": "1b3wYiXMf8Qc7aDgYdELQowr2lv_72XYd",
      "authorship_tag": "ABX9TyPAah3tL72ytqqFzluf5Ucr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZHOU-py/NLP/blob/master/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/Cornell_Movies_Dialogs_Corpus_S2S_%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhgY1QnSUEjB"
      },
      "source": [
        "#### 下载数据文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DynCqeQOUDvm"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32CYrlBG9Mxz"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K74sOYNdbj_"
      },
      "source": [
        "#### 加载和预处理数据\n",
        "- 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
        "- involves 9,035 characters from 617 movies\n",
        "- in total 304,713 utterances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKxDM889VRNa",
        "outputId": "da9c3ff4-6211-4379-ddc5-22924b21cabb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corpus_name = \"cornell movie-dialogs corpus\" \n",
        "corpus = os.path.join(\"data\", corpus_name)\n",
        "def printLines(file, n=10):\n",
        "  with open(file, 'rb') as datafile:\n",
        "    lines = datafile.readlines() \n",
        "  for line in lines[:n]:\n",
        "    print(line)\n",
        "printLines(os.path.join(corpus, \"movie_lines.txt\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
            "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
            "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
            "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
            "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
            "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
            "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
            "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
            "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
            "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMBdp3k02gcF"
      },
      "source": [
        "##### 创建格式化数据\n",
        "解析原始数据文件 movie_lines.txt\n",
        "* `loadLines`：将文件的每一行拆分为字段（lineID, characterID, movieID, character, text)组合的字典\n",
        "* `loadConversations`: 根据`movie_conversations.txt`将`loadLines`中的每一行数据进行归类\n",
        "* `extractSentencePairs`: 从对话中提取句子对\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-M2IVvWWIwO"
      },
      "source": [
        "# 将文件的每一行拆分为字段字典\n",
        "def loadLines(fileName, fields):\n",
        "  lines = {}\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\" +++$+++ \")\n",
        "      lineObj = {}\n",
        "      for i, field in enumerate(fields):\n",
        "        lineObj[field] = values[i]\n",
        "      lines[lineObj['lineID']] = lineObj\n",
        "  return lines"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfF5h6HPdA5w"
      },
      "source": [
        "#  将 'loadLines'中的行字段分组为基于 *movie_conversations.txt* 的对话\n",
        "def loadConversations(fileName, lines, fields):\n",
        "  conversations = []\n",
        "  with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\" +++$+++ \")\n",
        "      # Extract fields\n",
        "      convObj = {}\n",
        "      for i, field in enumerate(fields):\n",
        "        convObj[field] = values[i]\n",
        "      # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485','L598486', ...]\")\n",
        "      lineIds = eval(convObj[\"utteranceIDs\"])\n",
        "      # Reassemble lines\n",
        "      convObj[\"lines\"] = []\n",
        "#      print(lineIds)\n",
        "      for lineId in lineIds:\n",
        "       # print(lines[lineId])\n",
        "        if lines[lineId]:\n",
        "          convObj[\"lines\"].append(lines[lineId])\n",
        "        else:\n",
        "          continue\n",
        "      conversations.append(convObj)\n",
        "\n",
        "  return conversations"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCQmH3y5i-V"
      },
      "source": [
        "# 从对话中提取一对句子\n",
        "def extractSentencePairs(conversations):\n",
        "  qa_pairs = []\n",
        "  for conversation in conversations:\n",
        "    for i in range(len(conversation[\"lines\"])-1):\n",
        "      inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "      targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "      if inputLine and targetLine:\n",
        "        qa_pairs.append([inputLine,targetLine])\n",
        "  return qa_pairs"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evYeZatM8CRz",
        "outputId": "7c12579c-27fb-4c4b-a825-c0817f24a9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 定义新文件的路径\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\") \n",
        "delimiter = '\\t'\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "# 初始化行dict，对话列表和字段ID\n",
        "lines = {}\n",
        "conversations = []\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"] \n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "# 加载行和进程对话\n",
        "print(\"\\n Processing corpus...\")\n",
        "lines = loadLines(os.path.join(corpus,\"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "print(\"\\n Loading conversations...\")\n",
        "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\\\n",
        "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "# 写入新的csv文件\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "  writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n') \n",
        "  for pair in extractSentencePairs(conversations):\n",
        "    writer.writerow(pair)\n",
        "\n",
        "# 打印一个样本的行\n",
        "print(\"\\nSample lines from file:\") \n",
        "printLines(datafile)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Processing corpus...\n",
            "\n",
            " Loading conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
            "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
            "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xes_FfCCUKU2"
      },
      "source": [
        "for i,fields in enumerate(MOVIE_CONVERSATIONS_FIELDS):\n",
        "  print(i,fields)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bny_XSEdEmWw"
      },
      "source": [
        "#### 加载和清洗数据\n",
        "创建词汇表并将查询/响应句子加载到内存  \n",
        "通过数据集中的单词创建一个索引。  \n",
        "创建`Voc`类，以存储从单词到索引的映射，索引到单词的反向映射，每个单词的计数和总单词量。  \n",
        "这个类提供：  \n",
        "* 词汇表中添加单词的方法（`addWord`)  \n",
        "* 添加所有单词到句子中的方法（`addSentence`)\n",
        "* 清洗不常见的单词方法（`trim`)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb936tzk98JP"
      },
      "source": [
        "# 默认词向量\n",
        "PAD_token = 0 # Used for padding short sentences \n",
        "SOS_token = 1 # Start-of-sentence token \n",
        "EOS_token = 2 # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.trimmed = False\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} \n",
        "    self.num_words = 3 # Count SOS, EOS, PAD\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "  \n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.num_words \n",
        "      self.word2count[word] = 1 \n",
        "      self.index2word[self.num_words] = word \n",
        "      self.num_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "  # 删除低于特定计数阈值的单词 \n",
        "  def trim(self, min_count):\n",
        "    if self.trimmed: return\n",
        "    self.trimmed = True\n",
        "    \n",
        "    keep_words = []\n",
        "    \n",
        "    for k, v in self.word2count.items(): \n",
        "      if v >= min_count:\n",
        "        keep_words.append(k)\n",
        "    \n",
        "    print('keep_words {} / {} = {:.4f}'.format(\n",
        "      len(keep_words), len(self.word2index), len(keep_words) /\n",
        "    len(self.word2index) ))\n",
        "    \n",
        "    # 重初始化字典\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "\n",
        "    self.num_words = 3 # Count default tokens \n",
        "    for word in keep_words:\n",
        "        self.addWord(word)\n",
        "\n",
        "# 小写并删除非字母字符\n",
        "def normalizeString(s):\n",
        "  s = s.lower()\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) \n",
        "  return s\n",
        "# 使用字符串句子，返回单词索引的句子\n",
        "\n",
        "def indexesFromSentence(voc, sentence):\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c844DYAJIVmj"
      },
      "source": [
        "组装词汇表和查询/响应语句对。\n",
        "\n",
        "`unicodeToAscii`将unicode字符串转换为ASCII。将所有字母转换为小写字母并清洗掉除基本标点之外的所有非字母字符（`normalizaString`). 最后为了收敛，过滤掉长度大于MAX_LENGTH的句子（`filterParis`)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inpreGoTH4-J"
      },
      "source": [
        "MAX_LENGTH = 10 # Maximum sentence length to consider\n",
        "\n",
        "# 将Unicode字符串转换为纯ASCII，引用\n",
        "# https://stackoverflow.com/a/518232/2809427 \n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "  c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "# 初始化Voc对象 和 格式化pairs对话存放到list中 \n",
        "def readVocs(datafile, corpus_name):\n",
        "  print(\"Reading lines...\")\n",
        "  # Read the file and split into lines\n",
        "  lines = open(datafile, encoding='utf-8').read().strip().split('\\n') \n",
        "  # Split every line into pairs and normalize\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] \n",
        "  voc = Voc(corpus_name)\n",
        "  return voc, pairs\n",
        "\n",
        "# 如果对 'p' 中的两个句子都低于 MAX_LENGTH 阈值，则返回True \n",
        "def filterPair(p):\n",
        "  # Input sequences need to preserve the last word for EOS token\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# 过滤满足条件的 pairs 对话 \n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# 使用上面定义的函数，返回一个填充的voc对象和对列表\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "  print(\"Start preparing training data ...\")\n",
        "  voc, pairs = readVocs(datafile, corpus_name) \n",
        "  print(\"Read {!s} sentence pairs\".format(len(pairs))) \n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to {!s} sentence pairs\".format(len(pairs))) \n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1]) \n",
        "  print(\"Counted words:\", voc.num_words) \n",
        "  return voc, pairs\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aciphVFKl7K",
        "outputId": "3dd3a0c6-4ece-4b3d-9016-a37ba84af0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 加载/组装voc和对\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir) \n",
        "# 打印一些对进行验证\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "  print(pair)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 63436 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 17755\n",
            "\n",
            "pairs:\n",
            "['there .', 'where ?']\n",
            "['you have my word . as a gentleman', 'you re sweet .']\n",
            "['hi .', 'looks like things worked out tonight huh ?']\n",
            "['you know chastity ?', 'i believe we share an art instructor']\n",
            "['have fun tonight ?', 'tons']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['what good stuff ?', ' the real you . ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju8WSWVhPDUW"
      },
      "source": [
        "另一个加快收敛的方式是 去除词汇表中很少使用的单词。减少特征空间也会降低模型学习目标函数的难度。  \n",
        "* 使用`voc.trim`函数去除MIN_COUNT阈值以下的单词。  \n",
        "* 如果句子中包含词频过小的单词，整个句子也被过滤掉。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp33iRYMKrRE",
        "outputId": "fa96d7d6-1152-4508-c353-ff7955377dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MIN_COUNT = 3 # 修剪的最小字数阈值\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT): \n",
        "  # 修剪来自voc的MIN_COUNT下使用的单词 \n",
        "  voc.trim(MIN_COUNT)\n",
        "\n",
        "  # Filter out pairs with trimmed words \n",
        "  keep_pairs = []\n",
        "  for pair in pairs:\n",
        "    input_sentence = pair[0] \n",
        "    output_sentence = pair[1] \n",
        "    keep_input = True \n",
        "    keep_output = True\n",
        "    # 检查输入句子\n",
        "    for word in input_sentence.split(' '):\n",
        "      if word not in voc.word2index: \n",
        "        keep_input = False\n",
        "        break\n",
        "\n",
        "    # 检查输出句子\n",
        "    for word in output_sentence.split(' '):\n",
        "      if word not in voc.word2index: \n",
        "        keep_output = False\n",
        "        break\n",
        "    \n",
        "    # 只保留输入或输出句子中不包含修剪单词的对 \n",
        "    if keep_input and keep_output:\n",
        "      keep_pairs.append(pair)\n",
        "  print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), \\\n",
        "                                                              len(keep_pairs), \\\n",
        "                                                              len(keep_pairs) / len(pairs)))\n",
        "  return keep_pairs \n",
        "\n",
        "# 修剪voc和对\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keep_words 7700 / 17752 = 0.4338\n",
            "Trimmed from 63436 pairs to 52460, 0.8270 of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvMpcjeoQvFH"
      },
      "source": [
        "#### 为模型准备数据\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dcoe6LdQfy_"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KudxSKSlQtjr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}